\section*{DNNs as Gaussian Processes}
Given a linear layer \(F:\mathbb{R}^n \to \mathbb{R}^m\quad x \mapsto Wx\) with weights \(w_{ij} \sim \mathcal{N}(0, \frac{\gamma^2}{n})\) and given \(X = [x_1, \ldots, x_s]\) one gets the GP \(WX \sim \mathcal{N}(0, K)\) with \(K_{i\mu, j\nu} = \mathds{1}_{i=j}\frac{\gamma^2 \langle x_\mu, x_\nu \rangle}{n}\). In a DNN the l'th layer preactivation \(W^l X^{l-1}\) is not normal ($X^{l-1}$ is random). In the wide layer limit the CLT restores a GP for preactivations whose \(K\) can be computed recursively (numerically): $K_{i \mu, j \nu}^l = \mathds{1}_{i=j} \mathbb{E}[\sum_s w_{is}\phi(x_{s\mu}^{l-1}) \sum_t w_{it}\phi(x_{t\nu}^{l-1})]$ where $x^{l-1} \sim GP(0, K^{l-1})$ and $\phi$ denotes the activation function. The preactivation Gaussian Process of the output layer can then be conditioned for prediction with conditional mean $f^*(x) = k(x, (x_1, \ldots, x_s)) K^\dagger y$ and variance $k^*(x,x) = k(x,x) - k(x, (x_1, \ldots, x_s)) K^\dagger k(x, (x_1, \ldots, x_s))^T$.
